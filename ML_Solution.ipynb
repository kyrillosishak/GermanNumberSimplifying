{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMgaN6bI3ZO5UkWYoVHsGOq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyrillosishak/GermanNumberSimplifying/blob/main/ML_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach uses a two-phase strategy to create a model that can simplify numbers in German text (like converting \"324.620,22 Euro\" to \"etwa 325.000 Euro\").\n",
        "- Phase 1: Synthetic Data Generation\n",
        "Instead of manually creating training examples, the approach uses LLaMA 3 (a large language model) to generate thousands of diverse training examples. It's shown a few examples of how numbers should be simplified, and then asked to generate many more similar examples. This is clever because:\n",
        "\n",
        "It saves enormous time compared to manual data creation\n",
        "LLaMA 3 understands context and generates natural-sounding German sentences\n",
        "It can create diverse examples covering many different number formats and contexts\n",
        "\n",
        "- Phase 2: Training\n",
        "Once we have this large dataset, it's used to train a much smaller, specialized model (mT5) that focuses solely on number simplification. Think of it like having a master craftsman (LLaMA 3) teach a specific skill to an apprentice (mT5). The smaller model:\n",
        "\n",
        "Is faster and more efficient than LLaMA 3\n",
        "Specializes in just one task, doing it very well\n",
        "Can run on less powerful hardware\n",
        "\n",
        "This two-phase approach combines the best of both worlds:\n",
        "\n",
        "1. Uses a powerful model for data generation\n",
        "2. Uses a efficient model for the actual task\n",
        "\n",
        "Results in a practical, focused tool for number simplification\n",
        "\n",
        "It's like using a factory (LLaMA 3) to create training materials, then using those materials to train a specialized worker (mT5) who becomes very good at one specific job."
      ],
      "metadata": {
        "id": "JFoL2p6GeiNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1 : Synthetic Data Generation"
      ],
      "metadata": {
        "id": "b032OB0dfoWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_K2kG7sNvbt",
        "outputId": "ffab8b72-5797-410a-d3aa-ad8b2e2e8790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "4gNzcMeDNxxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcHDd8w_Nz-3",
        "outputId": "289e637c-44b6-4fee-cc09-e07876b9df1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id =  tokenizer.unk_token_id\n",
        "tokenizer.padding_side = 'left'"
      ],
      "metadata": {
        "id": "VAX7AjGtN2da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_name, quantization_config=bnb_config, device_map={\"\": 0}\n",
        ")\n",
        "#Configure the pad token in the model\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "h8zLbrRUN6NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig\n",
        "import re\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure pad_token_id is set\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Define the generation function\n",
        "def generate(instruction):\n",
        "    prompt = f\"<|begin_of_text|> {instruction} <|end_of_text|>\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    attention_mask = inputs[\"attention_mask\"].cuda()  # Explicitly pass attention_mask\n",
        "\n",
        "    # Define generation configuration\n",
        "    generation_config = GenerationConfig(\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        temperature=2.0,\n",
        "        top_p=1.0,\n",
        "        top_k=50,\n",
        "        num_beams=1,\n",
        "        return_legacy_cache=True  # Maintain legacy behavior if desired\n",
        "    )\n",
        "\n",
        "    # Generate output\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "\n",
        "    # Decode and return the output\n",
        "    outputs = []\n",
        "    for seq in generation_output.sequences:\n",
        "        outputs.append(tokenizer.decode(seq, skip_special_tokens=True))\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "ocdQC5kNN9an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data and template\n",
        "examples = \"\"\"\n",
        "This is examples of a sentence with numbers and simplification of this number:\n",
        "\n",
        "Input: 324.620,22 Euro wurden gespendet.\n",
        "Output: etwa 325.000 Euro wurden gespendet.\n",
        "\n",
        "Input: 1.897 Menschen nahmen teil.\n",
        "Output: etwa 2.000 Menschen nahmen teil.\n",
        "\n",
        "Input: 25 Prozent der Bevölkerung sind betroffen.\n",
        "Output: jeder Vierte der Bevölkerung sind betroffen.\n",
        "\n",
        "Input: 90 Prozent stimmten zu.\n",
        "Output: fast alle stimmten zu.\n",
        "\n",
        "Input: 14 Prozent lehnten ab.\n",
        "Output: wenige lehnten ab.\n",
        "\n",
        "Input: Bei 38,7 Grad Celsius ist es sehr heiß.\n",
        "Output: Bei etwa 39 Grad Celsius ist es sehr heiß.\n",
        "\n",
        "Input: denn die Rente steigt um 4,57 Prozent.\n",
        "Output: denn die Rente steigt um wenige.\n",
        "\n",
        "Input: Im Jahr 2024 gab es 1.234 Ereignisse.\n",
        "Output: Im Jahr 2024 gab es etwa 1.000 Ereignisse.\n",
        "\n",
        "Input: Am 1. Januar 2024 waren es 5.678 Teilnehmer.\n",
        "Output: Am 1. Januar 2024 waren es etwa 6.000 Teilnehmer.\n",
        "\n",
        "Input: Im Jahr 2025 gab es 2018 Ereignisse.\n",
        "Output: Im Jahr 2025 gab es etwa 2000 Ereignisse.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template_generate = f\"\"\"\n",
        "{examples}\n",
        "\n",
        "Generate a new 100 sentences with a number and simplify them in German language:\n",
        "Input:\n",
        "Output:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sKBD_UYFOCqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Input and Output pairs from the generated text\n",
        "def extract_pairs(text):\n",
        "    input_output_pattern = re.compile(r\"Input: (.+?)\\nOutput: (.+?)\\n\")\n",
        "    matches = input_output_pattern.findall(text)\n",
        "    return [{\"input\": match[0].strip(), \"output\": match[1].strip()} for match in matches]\n",
        "\n",
        "# Main function to run and collect data\n",
        "def collect_data(num_samples, prompt_template, batch_size=100):\n",
        "    collected_data = []\n",
        "\n",
        "    with tqdm(total=num_samples) as pbar:\n",
        "        while len(collected_data) < num_samples:\n",
        "            outputs = generate(prompt_template)\n",
        "            for output in outputs:\n",
        "                pairs = extract_pairs(output)\n",
        "                collected_data.extend(pairs)\n",
        "                pbar.update(len(pairs)) #update progress bar\n",
        "                if len(collected_data) >= num_samples:\n",
        "                    break\n",
        "\n",
        "    return collected_data[:num_samples]"
      ],
      "metadata": {
        "id": "Z5HtPdeXOHnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect 10,000 examples\n",
        "data = collect_data(num_samples=10000, prompt_template)\n",
        "\n",
        "# Save the data to a JSON file\n",
        "with open(\"simplified_numbers_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Collected {len(data)} input-output pairs and saved to 'simplified_numbers_data.json'\")"
      ],
      "metadata": {
        "id": "V5StabAlOpdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is example of generated data"
      ],
      "metadata": {
        "id": "GX9b2g3yeM-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Training\n",
        "\n",
        "Let's now train an encoder decoder model (T5)"
      ],
      "metadata": {
        "id": "2Vw2mGrveBM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "metadata": {
        "id": "XeDC16TreE8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset class\n",
        "class NumberToTextDataset(Dataset):\n",
        "    def __init__(self, texts, targets, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        target = str(self.targets[idx])\n",
        "\n",
        "        # Prepare input\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Prepare target\n",
        "        targets = self.tokenizer.encode_plus(\n",
        "            target,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': targets['input_ids'].squeeze()\n",
        "        }"
      ],
      "metadata": {
        "id": "peE_dW9SeGRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    # Initialize tokenizer and model\n",
        "    model_name = \"google/mt5-small\"  # You can also use \"google/mt5-base\" or larger models\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = NumberToTextDataset(\n",
        "        df['input'],\n",
        "        df['output'],\n",
        "        tokenizer\n",
        "    )\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # Training settings\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    num_epochs = 50\n",
        "    num_training_steps = num_epochs * len(dataloader)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    model.save_pretrained(\"number_to_text_model\")\n",
        "    tokenizer.save_pretrained(\"number_to_text_model\")\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "DJDvp8QbeLYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test the model\n",
        "def test_model(model, tokenizer, text):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "jDcshKoOeanP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = train_model()\n",
        "\n",
        "# Test the model\n",
        "test_input = \"324.620,22 Euro wurden gespendet.\"\n",
        "prediction = test_model(model, tokenizer, test_input)\n",
        "print(f\"Input: {test_input}\")\n",
        "print(f\"Prediction: {prediction}\")"
      ],
      "metadata": {
        "id": "8ybRvTcneeW5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}